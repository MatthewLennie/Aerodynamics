{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "import torch \n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import animation, rc\n",
    "rc('animation', html='jshtml')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(): #just doing it as a function to clean up whats in scope/ \n",
    "    df = pd.read_pickle(\"Reduced.pkl\")\n",
    "    Split = 0.8\n",
    "    SplitInd = int(Split * len(df))\n",
    "    LabelList = ['ts',*range(6)]\n",
    "    Input = torch.tensor(df[LabelList].values)\n",
    "    Output = torch.tensor(df['Labels'].values)\n",
    "\n",
    "    #load second file hacky way for now. Only two files needed for training set.  \n",
    "    df3 = pd.read_pickle(\"Reduced3.pkl\")\n",
    "\n",
    "    Input3 = torch.tensor(df3[LabelList].values)\n",
    "    Output3 = torch.tensor(df3['Labels'].values)\n",
    "\n",
    "    df4 = pd.read_pickle(\"Sensor0-Copy1\")\n",
    "    df4.Labels = 1\n",
    "\n",
    "    Input4 = torch.tensor(df4[LabelList].values)\n",
    "    Output4 = torch.tensor(df4['Labels'].values)\n",
    "\n",
    "\n",
    "    #Valid trainSplit\n",
    "    InpTrain4, InpValid4 = Input4[:SplitInd], Input4[SplitInd:]\n",
    "    OutTrain4, OutValid4 = Output4[:SplitInd], Output4[SplitInd:]\n",
    "\n",
    "\n",
    "    #Valid trainSplit\n",
    "    InpTrain, InpValid = Input[:SplitInd], Input[SplitInd:]\n",
    "    OutTrain, OutValid = Output[:SplitInd], Output[SplitInd:]\n",
    "\n",
    "    #Valid trainSplit\n",
    "    InpTrain3, InpValid3 = Input3[:SplitInd], Input3[SplitInd:]\n",
    "    OutTrain3, OutValid3 = Output3[:SplitInd], Output3[SplitInd:]\n",
    "\n",
    "    #Concat two files. \n",
    "    InputTrain = torch.cat((InpTrain,InpTrain3,InpTrain4),0)\n",
    "    InputValid = torch.cat((InpValid,InpValid3,InpValid4),0)\n",
    "    OutputTrain = torch.cat((OutTrain,OutTrain3,OutTrain4),0)\n",
    "    OutputValid = torch.cat((OutValid,OutValid3,OutValid4),0)\n",
    "\n",
    "    #Train scaler on training data only. No peeking now!! \n",
    "    InpScaler = StandardScaler()\n",
    "    InpScaler.fit(InputTrain)\n",
    "\n",
    "\n",
    "\n",
    "    InputTrain = torch.tensor(InpScaler.transform(InputTrain))\n",
    "    InputValid = torch.tensor(InpScaler.transform(InputValid))\n",
    "\n",
    "    BatchSize = 200000\n",
    "    #Into DataLoaders\n",
    "    # Large Batch size seems to perform better. \n",
    "    TrainDataSet = torch.utils.data.TensorDataset(InputTrain, OutputTrain)\n",
    "    Train = torch.utils.data.DataLoader(TrainDataSet,batch_size = BatchSize,num_workers = 1)\n",
    "\n",
    "    ValidDataSet = torch.utils.data.TensorDataset(InputValid, OutputValid)\n",
    "    Valid = torch.utils.data.DataLoader(TrainDataSet,batch_size = BatchSize,num_workers = 1)\n",
    "    LenTrain = len(InpTrain)\n",
    "    LenValid = len(InpValid)\n",
    "    Inputs  = {\"train\": InputTrain , \"valid\" : InputValid}\n",
    "    Outputs = {\"train\": OutputTrain , \"valid\": OutputValid}\n",
    "    return Inputs, Outputs\n",
    "    #return Train, Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inputs, Outputs = LoadData()\n",
    "#Inputs  = {\"Train\": InputTrain , \"Valid\" : InputValid}\n",
    "#Outputs = {\"Train\": OutputTrain , \"Valid\": OutputValid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"Reduced.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>Labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.100506</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017518</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.112558</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017520</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.122963</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017525</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.124815</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017534</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>-0.114058</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017546</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>-0.106800</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017562</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.111124</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017581</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.109121</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017603</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>-0.103176</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017627</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>-0.101728</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017655</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-0.101535</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017686</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>-0.100615</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017719</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>-0.109701</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017754</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>-0.115534</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017792</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>-0.118585</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017831</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>-0.124403</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017873</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-0.122652</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017916</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>-0.116497</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017960</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>-0.115469</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018006</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-0.113153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018052</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-0.107262</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018100</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>-0.106966</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018147</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>-0.101446</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018195</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>-0.095448</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018243</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>-0.088291</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018291</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.085763</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018339</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>-0.077268</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018386</td>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>-0.074495</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018432</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>-0.077641</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018477</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>-0.081948</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018520</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499520</th>\n",
       "      <td>-0.144279</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012306</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499536</th>\n",
       "      <td>-0.143114</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012499</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499552</th>\n",
       "      <td>-0.137471</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012701</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499568</th>\n",
       "      <td>-0.131158</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012910</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499584</th>\n",
       "      <td>-0.117526</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.013125</td>\n",
       "      <td>0.001702</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499600</th>\n",
       "      <td>-0.112188</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499616</th>\n",
       "      <td>-0.120314</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.013571</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499632</th>\n",
       "      <td>-0.117544</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.013799</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499648</th>\n",
       "      <td>-0.118681</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014028</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499664</th>\n",
       "      <td>-0.119177</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014259</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499680</th>\n",
       "      <td>-0.112830</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014489</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499696</th>\n",
       "      <td>-0.109464</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014717</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499712</th>\n",
       "      <td>-0.097705</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014942</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000040</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499728</th>\n",
       "      <td>-0.087507</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015164</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499744</th>\n",
       "      <td>-0.075431</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015380</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499760</th>\n",
       "      <td>-0.062847</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015589</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499776</th>\n",
       "      <td>-0.050763</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015791</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499792</th>\n",
       "      <td>-0.039783</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015984</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499808</th>\n",
       "      <td>-0.036454</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016168</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499824</th>\n",
       "      <td>-0.037477</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016341</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499840</th>\n",
       "      <td>-0.037959</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016502</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499856</th>\n",
       "      <td>-0.043261</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016651</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499872</th>\n",
       "      <td>-0.040946</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016786</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499888</th>\n",
       "      <td>-0.040069</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.016907</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499904</th>\n",
       "      <td>-0.031454</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017014</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499920</th>\n",
       "      <td>-0.040827</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017105</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499936</th>\n",
       "      <td>-0.045184</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017180</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499952</th>\n",
       "      <td>-0.056184</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017238</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499968</th>\n",
       "      <td>-0.065952</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017280</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4499984</th>\n",
       "      <td>-0.074494</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.017305</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.000072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281250 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ts  Labels         0         1         2         3         4  \\\n",
       "0       -0.100506       0 -0.017518  0.001688  0.000334  0.000028 -0.000025   \n",
       "16      -0.112558       0 -0.017520  0.001688  0.000334  0.000028 -0.000025   \n",
       "32      -0.122963       0 -0.017525  0.001688  0.000334  0.000028 -0.000025   \n",
       "48      -0.124815       0 -0.017534  0.001687  0.000334  0.000028 -0.000025   \n",
       "64      -0.114058       0 -0.017546  0.001687  0.000334  0.000028 -0.000025   \n",
       "80      -0.106800       0 -0.017562  0.001687  0.000334  0.000028 -0.000025   \n",
       "96      -0.111124       0 -0.017581  0.001686  0.000334  0.000028 -0.000025   \n",
       "112     -0.109121       0 -0.017603  0.001686  0.000334  0.000028 -0.000025   \n",
       "128     -0.103176       0 -0.017627  0.001685  0.000334  0.000028 -0.000025   \n",
       "144     -0.101728       0 -0.017655  0.001684  0.000334  0.000028 -0.000025   \n",
       "160     -0.101535       0 -0.017686  0.001684  0.000334  0.000028 -0.000025   \n",
       "176     -0.100615       0 -0.017719  0.001683  0.000334  0.000028 -0.000025   \n",
       "192     -0.109701       0 -0.017754  0.001682  0.000334  0.000028 -0.000025   \n",
       "208     -0.115534       0 -0.017792  0.001681  0.000334  0.000028 -0.000025   \n",
       "224     -0.118585       0 -0.017831  0.001680  0.000334  0.000028 -0.000025   \n",
       "240     -0.124403       0 -0.017873  0.001679  0.000334  0.000028 -0.000025   \n",
       "256     -0.122652       0 -0.017916  0.001678  0.000334  0.000028 -0.000025   \n",
       "272     -0.116497       0 -0.017960  0.001677  0.000334  0.000029 -0.000025   \n",
       "288     -0.115469       0 -0.018006  0.001676  0.000334  0.000029 -0.000025   \n",
       "304     -0.113153       0 -0.018052  0.001675  0.000334  0.000029 -0.000025   \n",
       "320     -0.107262       0 -0.018100  0.001674  0.000333  0.000029 -0.000025   \n",
       "336     -0.106966       0 -0.018147  0.001674  0.000333  0.000029 -0.000025   \n",
       "352     -0.101446       0 -0.018195  0.001673  0.000333  0.000029 -0.000025   \n",
       "368     -0.095448       0 -0.018243  0.001672  0.000333  0.000030 -0.000025   \n",
       "384     -0.088291       0 -0.018291  0.001671  0.000332  0.000030 -0.000024   \n",
       "400     -0.085763       0 -0.018339  0.001670  0.000332  0.000030 -0.000024   \n",
       "416     -0.077268       0 -0.018386  0.001669  0.000332  0.000030 -0.000024   \n",
       "432     -0.074495       0 -0.018432  0.001668  0.000331  0.000030 -0.000024   \n",
       "448     -0.077641       0 -0.018477  0.001668  0.000331  0.000031 -0.000024   \n",
       "464     -0.081948       0 -0.018520  0.001667  0.000330  0.000031 -0.000024   \n",
       "...           ...     ...       ...       ...       ...       ...       ...   \n",
       "4499520 -0.144279       0 -0.012306  0.001731  0.000402  0.000027 -0.000037   \n",
       "4499536 -0.143114       0 -0.012499  0.001724  0.000403  0.000027 -0.000037   \n",
       "4499552 -0.137471       0 -0.012701  0.001717  0.000404  0.000028 -0.000037   \n",
       "4499568 -0.131158       0 -0.012910  0.001709  0.000406  0.000028 -0.000037   \n",
       "4499584 -0.117526       0 -0.013125  0.001702  0.000407  0.000029 -0.000038   \n",
       "4499600 -0.112188       0 -0.013346  0.001694  0.000408  0.000030 -0.000038   \n",
       "4499616 -0.120314       0 -0.013571  0.001686  0.000409  0.000030 -0.000038   \n",
       "4499632 -0.117544       0 -0.013799  0.001678  0.000411  0.000031 -0.000039   \n",
       "4499648 -0.118681       0 -0.014028  0.001670  0.000412  0.000031 -0.000039   \n",
       "4499664 -0.119177       0 -0.014259  0.001663  0.000413  0.000032 -0.000039   \n",
       "4499680 -0.112830       0 -0.014489  0.001655  0.000414  0.000032 -0.000040   \n",
       "4499696 -0.109464       0 -0.014717  0.001647  0.000415  0.000033 -0.000040   \n",
       "4499712 -0.097705       0 -0.014942  0.001639  0.000416  0.000034 -0.000040   \n",
       "4499728 -0.087507       0 -0.015164  0.001632  0.000417  0.000034 -0.000041   \n",
       "4499744 -0.075431       0 -0.015380  0.001624  0.000418  0.000035 -0.000041   \n",
       "4499760 -0.062847       0 -0.015589  0.001617  0.000419  0.000035 -0.000041   \n",
       "4499776 -0.050763       0 -0.015791  0.001611  0.000420  0.000036 -0.000042   \n",
       "4499792 -0.039783       0 -0.015984  0.001604  0.000421  0.000036 -0.000042   \n",
       "4499808 -0.036454       0 -0.016168  0.001598  0.000422  0.000037 -0.000042   \n",
       "4499824 -0.037477       0 -0.016341  0.001592  0.000423  0.000037 -0.000043   \n",
       "4499840 -0.037959       0 -0.016502  0.001587  0.000424  0.000038 -0.000043   \n",
       "4499856 -0.043261       0 -0.016651  0.001582  0.000424  0.000038 -0.000043   \n",
       "4499872 -0.040946       0 -0.016786  0.001578  0.000425  0.000038 -0.000043   \n",
       "4499888 -0.040069       0 -0.016907  0.001574  0.000425  0.000039 -0.000044   \n",
       "4499904 -0.031454       0 -0.017014  0.001570  0.000426  0.000039 -0.000044   \n",
       "4499920 -0.040827       0 -0.017105  0.001567  0.000426  0.000039 -0.000044   \n",
       "4499936 -0.045184       0 -0.017180  0.001564  0.000427  0.000039 -0.000044   \n",
       "4499952 -0.056184       0 -0.017238  0.001563  0.000427  0.000039 -0.000044   \n",
       "4499968 -0.065952       0 -0.017280  0.001561  0.000427  0.000039 -0.000044   \n",
       "4499984 -0.074494       0 -0.017305  0.001560  0.000427  0.000040 -0.000044   \n",
       "\n",
       "                5  \n",
       "0        0.000065  \n",
       "16       0.000065  \n",
       "32       0.000065  \n",
       "48       0.000065  \n",
       "64       0.000065  \n",
       "80       0.000065  \n",
       "96       0.000065  \n",
       "112      0.000065  \n",
       "128      0.000065  \n",
       "144      0.000065  \n",
       "160      0.000065  \n",
       "176      0.000065  \n",
       "192      0.000065  \n",
       "208      0.000066  \n",
       "224      0.000066  \n",
       "240      0.000066  \n",
       "256      0.000066  \n",
       "272      0.000066  \n",
       "288      0.000066  \n",
       "304      0.000066  \n",
       "320      0.000066  \n",
       "336      0.000066  \n",
       "352      0.000066  \n",
       "368      0.000066  \n",
       "384      0.000066  \n",
       "400      0.000066  \n",
       "416      0.000066  \n",
       "432      0.000067  \n",
       "448      0.000067  \n",
       "464      0.000067  \n",
       "...           ...  \n",
       "4499520  0.000077  \n",
       "4499536  0.000077  \n",
       "4499552  0.000077  \n",
       "4499568  0.000077  \n",
       "4499584  0.000077  \n",
       "4499600  0.000077  \n",
       "4499616  0.000076  \n",
       "4499632  0.000076  \n",
       "4499648  0.000076  \n",
       "4499664  0.000076  \n",
       "4499680  0.000075  \n",
       "4499696  0.000075  \n",
       "4499712  0.000075  \n",
       "4499728  0.000075  \n",
       "4499744  0.000074  \n",
       "4499760  0.000074  \n",
       "4499776  0.000074  \n",
       "4499792  0.000074  \n",
       "4499808  0.000073  \n",
       "4499824  0.000073  \n",
       "4499840  0.000073  \n",
       "4499856  0.000073  \n",
       "4499872  0.000073  \n",
       "4499888  0.000073  \n",
       "4499904  0.000072  \n",
       "4499920  0.000072  \n",
       "4499936  0.000072  \n",
       "4499952  0.000072  \n",
       "4499968  0.000072  \n",
       "4499984  0.000072  \n",
       "\n",
       "[281250 rows x 8 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#p = iter(range(1000000000))\n",
    "for case in [\"train\",\"valid\"]:\n",
    "    group = pd.DataFrame(Inputs[case].numpy()).groupby(np.arange(len(Inputs[case].numpy()))//7)\n",
    "    groupLabels = pd.DataFrame(Outputs[case].numpy()).groupby(np.arange(len(Outputs[case].numpy()))//7)\n",
    "    for g, gl in zip(group,groupLabels):\n",
    "        if g[1].shape[0] ==7:\n",
    "            Lab = int(np.round_(gl[1].mean()))\n",
    "            single = g[1]\n",
    "            singleL = gl[1]\n",
    "            im = Image.fromarray(np.uint8((single.values+1)*125))\n",
    "            #print(\"./Pictures/{}/{}/{}.jpeg\".format(case,Lab,next(p)))\n",
    "            im.save(\"./Pictures/{}/{}/{}.jpeg\".format(case,Lab,single.index[3]))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'168745'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(single.index[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.metrics import error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageDataBunch.from_folder(\"./Pictures/\", size=49, bs=1024).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=ImageDataBunch;\n",
       "\n",
       "Train: LabelList (96428 items)\n",
       "x: ImageItemList\n",
       "Image (3, 49, 49),Image (3, 49, 49),Image (3, 49, 49),Image (3, 49, 49),Image (3, 49, 49)\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: Pictures;\n",
       "\n",
       "Valid: LabelList (24117 items)\n",
       "x: ImageItemList\n",
       "Image (3, 49, 49),Image (3, 49, 49),Image (3, 49, 49),Image (3, 49, 49),Image (3, 49, 49)\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: Pictures;\n",
       "\n",
       "Test: None, model=Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): Sequential(\n",
       "    (0): AdaptiveConcatPool2d(\n",
       "      (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "      (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "    )\n",
       "    (1): Flatten()\n",
       "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.25)\n",
       "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.5)\n",
       "    (8): Linear(in_features=512, out_features=2, bias=True)\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function error_rate at 0x7fb93ed631e0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('Pictures'), model_dir='models', callback_fns=[<class 'fastai.basic_train.Recorder'>], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace)\n",
       "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ReLU(inplace)\n",
       "  (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (15): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (16): ReLU(inplace)\n",
       "  (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (21): ReLU(inplace)\n",
       "  (22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (24): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (25): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (26): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (27): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (28): ReLU(inplace)\n",
       "  (29): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (31): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (32): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (33): ReLU(inplace)\n",
       "  (34): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (35): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (36): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (37): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (38): ReLU(inplace)\n",
       "  (39): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (40): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (9): ReLU(inplace)\n",
       "  (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (14): ReLU(inplace)\n",
       "  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (16): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (19): ReLU(inplace)\n",
       "  (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (23): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (24): ReLU(inplace)\n",
       "  (25): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (26): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (27): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (28): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (29): ReLU(inplace)\n",
       "  (30): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (32): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (33): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (34): ReLU(inplace)\n",
       "  (35): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (36): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (37): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "  (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (39): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (41): ReLU(inplace)\n",
       "  (42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (43): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (44): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (45): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (46): ReLU(inplace)\n",
       "  (47): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (48): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "), Sequential(\n",
       "  (0): AdaptiveAvgPool2d(output_size=1)\n",
       "  (1): AdaptiveMaxPool2d(output_size=1)\n",
       "  (2): Flatten()\n",
       "  (3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): Dropout(p=0.25)\n",
       "  (5): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (6): ReLU(inplace)\n",
       "  (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (8): Dropout(p=0.5)\n",
       "  (9): Linear(in_features=512, out_features=2, bias=True)\n",
       ")])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = create_cnn(data, models.resnet34, metrics=error_rate)\n",
    "#learn = ConvLearner.pretrained(resnet34, data, precompute=False)\n",
    "learn.load(\"stage-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict() missing 1 required positional argument: 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-184-17c52fb00833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: predict() missing 1 required positional argument: 'item'"
     ]
    }
   ],
   "source": [
    "learn.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Aerodynamics/Pictures/Valid/Turb26.jpeg'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label='Turb'\n",
    "\"./Aerodynamics/Pictures/{}/{}{}.jpeg\".format(case,Label,next(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 168747, 168748, 168749])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(Inputs[case].numpy()//7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p = iter(range(1000000000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.387673</td>\n",
       "      <td>-0.548766</td>\n",
       "      <td>0.454312</td>\n",
       "      <td>0.270735</td>\n",
       "      <td>0.033641</td>\n",
       "      <td>-0.047244</td>\n",
       "      <td>0.122374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.409025</td>\n",
       "      <td>-0.550159</td>\n",
       "      <td>0.454115</td>\n",
       "      <td>0.270353</td>\n",
       "      <td>0.034052</td>\n",
       "      <td>-0.047040</td>\n",
       "      <td>0.122451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.409748</td>\n",
       "      <td>-0.551508</td>\n",
       "      <td>0.453935</td>\n",
       "      <td>0.269944</td>\n",
       "      <td>0.034489</td>\n",
       "      <td>-0.046833</td>\n",
       "      <td>0.122517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.423944</td>\n",
       "      <td>-0.552806</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.269510</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>-0.046624</td>\n",
       "      <td>0.122570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.443765</td>\n",
       "      <td>-0.554051</td>\n",
       "      <td>0.453630</td>\n",
       "      <td>0.269052</td>\n",
       "      <td>0.035435</td>\n",
       "      <td>-0.046415</td>\n",
       "      <td>0.122612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.454950</td>\n",
       "      <td>-0.555236</td>\n",
       "      <td>0.453505</td>\n",
       "      <td>0.268572</td>\n",
       "      <td>0.035942</td>\n",
       "      <td>-0.046208</td>\n",
       "      <td>0.122641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.451069</td>\n",
       "      <td>-0.556359</td>\n",
       "      <td>0.453399</td>\n",
       "      <td>0.268072</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>-0.046004</td>\n",
       "      <td>0.122658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6\n",
       "28 -0.387673 -0.548766  0.454312  0.270735  0.033641 -0.047244  0.122374\n",
       "29 -0.409025 -0.550159  0.454115  0.270353  0.034052 -0.047040  0.122451\n",
       "30 -0.409748 -0.551508  0.453935  0.269944  0.034489 -0.046833  0.122517\n",
       "31 -0.423944 -0.552806  0.453773  0.269510  0.034950 -0.046624  0.122570\n",
       "32 -0.443765 -0.554051  0.453630  0.269052  0.035435 -0.046415  0.122612\n",
       "33 -0.454950 -0.555236  0.453505  0.268572  0.035942 -0.046208  0.122641\n",
       "34 -0.451069 -0.556359  0.453399  0.268072  0.036471 -0.046004  0.122658"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.19204231e-02, -1.41875517e-03, -1.24863083e-03,\n",
       "         5.32760650e-04, -6.31041158e-04, -1.21721040e-03],\n",
       "       [ 7.71384876e-03, -1.66468252e-03,  9.55475341e-04,\n",
       "        -7.55148391e-06,  1.15938645e-03, -1.99241947e-04],\n",
       "       [ 4.00082896e-03,  2.75497300e-04, -1.14362812e-03,\n",
       "         2.22457951e-04,  6.11623207e-04, -6.42242931e-04],\n",
       "       [ 5.02413731e-03,  3.21718366e-04, -9.72154586e-04,\n",
       "        -2.93755578e-04,  2.83980630e-04, -7.95072380e-04],\n",
       "       [ 1.81729134e-02,  2.83827985e-03,  7.14678212e-05,\n",
       "        -8.12619016e-06, -1.33868880e-05, -8.26416955e-06],\n",
       "       [-1.04257602e-02,  1.96335281e-03,  2.76281534e-04,\n",
       "         2.56687016e-05, -3.92590302e-05,  7.06700492e-05]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single[[0,1,2,3,4,5]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>Labels</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2813.000000</td>\n",
       "      <td>2813.000000</td>\n",
       "      <td>2813.000000</td>\n",
       "      <td>2813.000000</td>\n",
       "      <td>2813.000000</td>\n",
       "      <td>2813.000000</td>\n",
       "      <td>2813.000000</td>\n",
       "      <td>2813.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.005820</td>\n",
       "      <td>0.552791</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.203263</td>\n",
       "      <td>0.497294</td>\n",
       "      <td>0.029613</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.784927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020724</td>\n",
       "      <td>-0.031024</td>\n",
       "      <td>-0.006357</td>\n",
       "      <td>-0.005966</td>\n",
       "      <td>-0.004569</td>\n",
       "      <td>-0.003433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.102252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016771</td>\n",
       "      <td>-0.001256</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.004013</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009473</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.101748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.051267</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320739</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.005982</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ts       Labels            0            1            2  \\\n",
       "count  2813.000000  2813.000000  2813.000000  2813.000000  2813.000000   \n",
       "mean      0.005820     0.552791    -0.000104    -0.000031     0.000006   \n",
       "std       0.203263     0.497294     0.029613     0.003566     0.001315   \n",
       "min      -0.784927     0.000000    -0.020724    -0.031024    -0.006357   \n",
       "25%      -0.102252     0.000000    -0.016771    -0.001256    -0.000068   \n",
       "50%      -0.004013     1.000000    -0.009473     0.001594     0.000290   \n",
       "75%       0.101748     1.000000     0.003521     0.001806     0.000350   \n",
       "max       1.051267     1.000000     0.320739     0.012855     0.013900   \n",
       "\n",
       "                 3            4            5  \n",
       "count  2813.000000  2813.000000  2813.000000  \n",
       "mean      0.000012     0.000010     0.000002  \n",
       "std       0.000790     0.000616     0.000507  \n",
       "min      -0.005966    -0.004569    -0.003433  \n",
       "25%      -0.000030    -0.000058    -0.000031  \n",
       "50%       0.000037    -0.000027     0.000068  \n",
       "75%       0.000088     0.000063     0.000075  \n",
       "max       0.010486     0.005982     0.003750  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
